# 小笔记

- [机器学习特征选择的方法](https://www.cnblogs.com/bonelee/p/8632866.html)
- [sklearn聚类使用](https://www.cnblogs.com/little-horse/p/7367407.html)
- [典型推荐算法总结](https://blog.csdn.net/u011095110/article/details/84403564)
- [机器学习算法总结](https://www.cnblogs.com/jiangxinyang/p/9217424.html)
- [浅谈机器学习方法](https://www.cnblogs.com/flippedkiki/p/7209076.html?utm_source=itdadao&utm_medium=referral)
- [Bagging和Boosting 概念及区别](https://www.cnblogs.com/gczr/p/7097442.html)
- [机器学习面试问题大概梳理](https://www.cnblogs.com/gczr/p/6829176.html)
- [Keras学习率调整](https://www.cnblogs.com/nxf-rabbit75/p/10564888.html)
- [网格搜索参数详解](https://blog.csdn.net/CherDW/article/details/54970366)
- [多标签分类、多任务分类、多输出回归概念](https://blog.csdn.net/zb1165048017/article/details/77882600)
- [详解机器学习中的熵、条件熵、相对熵和交叉熵](https://www.cnblogs.com/kyrieng/p/8694705.html)
- [集成学习调参]( https://blog.csdn.net/kylin_learn/article/details/82192045 )
- [详解机器学习中的熵、联合熵、条件熵、相对熵和交叉熵](https://www.cnblogs.com/zzdbullet/p/10075347.html)
- [偏差、方差和误差](https://www.zhihu.com/question/27068705)

GBDT、XGBoost、lightBoost
- [XGBoost](https://www.hrwhisper.me/machine-learning-xgboost/)
- [GBDT梯度提升树](https://blog.csdn.net/weixin_42933718/article/details/88421574#GBDT_2)
- [机器学习-树模型理论（GDBT，xgboost，lightBoost，随机森林）](https://www.cnblogs.com/onemorepoint/p/9799124.html)
- [提升树、GBDT与XGBoost](https://mp.weixin.qq.com/s/ePHvIADfBBhW8hlsmvidrQ)

机器学习实战参考资料：

- [随机森林代码与调参](https://www.jianshu.com/p/5354f2a42a73)，[随机森林的参数说明](https://www.cnblogs.com/gczr/p/7141712.html)
- [逻辑回归的参数说明](https://blog.csdn.net/sun_shengyun/article/details/53811483)，[逻辑回归调参](https://www.jianshu.com/p/99ceb640efc5)，[逻辑回归代码与调参](https://www.cnblogs.com/onemorepoint/p/9486998.html?utm_source=debugrun&utm_medium=referral)
- [sklearn常见分类器(二分类模板)](https://www.cnblogs.com/caiyishuai/p/11385825.html)
- [python sklearn常用分类算法模型的调用](https://my.oschina.net/tantexian/blog/1919888)

Java面试算法必须掌握：

- [Java面试-排序算法](https://blog.csdn.net/weixin_41835916/article/details/81661314)
- [Java面试-查找算法](https://blog.csdn.net/babylorin/article/details/67638156)
- [排序算法整合（冒泡，快速，希尔，拓扑，归并）](https://blog.csdn.net/onceing/article/details/99838520)
- [**动画图解：十大经典排序算法动画与解析**](https://blog.csdn.net/kexuanxiu1163/article/details/103051357)

## 1. 机器学习原理和公式推导：

- [决策树](https://blog.csdn.net/qq_24519677/article/details/82084718)

### 1.1 Adaboost

- [Adaboost公式推导1](https://www.jianshu.com/p/0d850d85dcbd)、[Adaboost公式推导2](https://zhuanlan.zhihu.com/p/62037189)
- [Adaboost原理1](https://www.cnblogs.com/ScorpioLu/p/8295990.html)、[Adaboost原理2](https://blog.csdn.net/qq_24519677/article/details/81910112)
- [Adaboost改进](https://www.jianshu.com/p/0bfce1806235)

## Pandas技巧

### 1. [dropna](https://blog.csdn.net/weixin_40283816/article/details/84304055)

```python
data.dropna(how = 'all')    # 传入这个参数后将只丢弃全为缺失值的那些行
data.dropna(axis = 1)       # 丢弃有缺失值的列（一般不会这么做，这样会删掉一个特征）
data.dropna(axis=1,how="all")   # 丢弃全为缺失值的那些列
data.dropna(axis=0,subset = ["Age", "Sex"])   # 丢弃‘Age’和‘Sex’这两列中有缺失值的行    
```



## 机器学习小知识点

### 评价指标

- [宏平均和微平均的对比](https://blog.csdn.net/Candy_GL/article/details/83059217)
- [sklearn中 F1-micro 与 F1-macro区别和计算原理](https://blog.csdn.net/lyb3b3b/article/details/84819931)
- [precision和accuracy的区别](https://blog.csdn.net/Du_Shuang/article/details/84073584)

### 数据不均衡

- [正负样本不均衡的解决办法](https://blog.csdn.net/jemila/article/details/77992967)
- [class_weight和sample_weight区别]( https://blog.csdn.net/weixin_40755306/article/details/82290033 )

### 支持向量机

- Support Vector Machine, SVM

- [SVM时间复杂度](https://blog.csdn.net/iteye_12567/article/details/81921956)

  ```python
  sklearn.svm.SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto_deprecated’, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)
  ```

- **C**：惩罚系数，理解为调节优化方向中两个指标（间隔大小，分类准确度）偏好的权重，即对误差的宽容度，**C越高，说明越不能容忍出现误差,容易过拟合，C越小，容易欠拟合，C过大或过小，泛化能力变差**。

- **gamma**：是选择RBF函数作为kernel后，该函数自带的一个参数。隐含地决定了数据映射到新的特征空间后的分布，**gamma越大，支持向量越少，gamma值越小，支持向量越多**。支持向量的个数影响训练与预测的速度。

### 聚类

#### 1. 均值漂移（MeanShift）

- 参考链接：[通俗理解Meanshift均值漂移算法](https://www.cnblogs.com/lowbi/p/10733733.html)，[Meanshift Algorithm](http://www.chioka.in/meanshift-algorithm-for-the-rest-of-us-python/)

- 算法复杂度$O(kn^2)$，其中n是数据点的数量，k是Meanshift的迭代次数。

- **概述**

  Mean-shift（均值迁移）的基本思想：在数据集中选定一个点，然后以这个点为圆心，r为半径，画一个圆(二维下是圆)，求出这个点到所有点的向量的平均值，而圆心与向量均值的和为新的圆心，然后迭代此过程，直到满足一点的条件结束。

  后来在此基础上加入了**核函数和权重系数**，使得Mean-shift 算法开始流行起来。目前它在**聚类、图像平滑、分割、跟踪**等方面有着广泛的应用。

- **图解过程**

  第一张图有一个子中心点,她向四周最近的点开始寻找,找到圆心与向量均值的和为新的圆心,然后依次循环,直到满足条件,则不会再寻找其他圆心点。

  ![img](https://img2018.cnblogs.com/blog/1567636/201904/1567636-20190419074346180-717338129.png)

- **Mean-shift 算法函数**

  - 核心函数：sklearn.cluster.MeanShift(核函数：RBF核函数)

    由上图可知，圆心(或种子)的确定和半径(或带宽)的选择，是影响算法效率的两个主要因素。所以在sklearn.cluster.MeanShift中重点说明了这两个参数的设定问题。

  - 主要参数

    - bandwidth：半径(或带宽)，float型。如果没有给出，则使用sklearn.cluster.estimate_bandwidth计算出半径(带宽)。（可选）
    - seeds：圆心（或种子），数组类型，即初始化的圆心。（可选）
    - bin_seeding：布尔值。如果为真，初始内核位置不是所有点的位置，而是点的离散版本的位置，其中点被分类到其粗糙度对应于带宽的网格上。将此选项设置为True将加速算法，因为较少的种子将被初始化。默认值：False.如果种子参数(seeds)不为None则忽略。

  - 主要属性

    - cluster_centers_：数组类型。计算出的聚类中心的坐标。
    - labels_：数组类型。每个数据点的分类标签。

- **核函数**

  核函数只是用来计算映射到高维空间之后的内积的一种简便方法，目的为让低维的不可分数据变成高维可分。利用核函数，可以忽略映射关系，直接在低维空间中完成计算。

- **引入核函数的偏移均值**

  在均值漂移中引入核函数的概念，能够使计算中**距离中心的点具有更大的权值**，反映距离越短，权值越大的特性。

  